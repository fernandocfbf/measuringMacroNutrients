{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import imread\n",
    "import scipy\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y =  train_test_split(x, y, test_size=0.33, random_state=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PIZZAS in training: 660\n",
      "Number of NON-PIZZAS in training: 343\n",
      "Number of PIZZAS in test: 313\n",
      "Number of NON-PIZZAS in test: 182\n"
     ]
    }
   ],
   "source": [
    "tr = np.array(train_y)\n",
    "ts = np.array(test_y)\n",
    "\n",
    "print(\"Number of PIZZAS in training: {}\".format(len(np.where(tr == 1)[0])))\n",
    "print(\"Number of NON-PIZZAS in training: {}\".format(len(np.where(tr == 0)[0])))\n",
    "\n",
    "print(\"Number of PIZZAS in test: {}\".format(len(np.where(ts == 1)[0])))\n",
    "print(\"Number of NON-PIZZAS in test: {}\".format(len(np.where(ts == 0)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig = np.array(train_x)\n",
    "test_x_orig = np.array(test_x)\n",
    "train_y = np.array([train_y])\n",
    "test_y = np.array([test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten\n",
    "test_x = test_x_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(AL, Y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    matrix_sum = 0 #initialize the sum at zero\n",
    "    \n",
    "    for key in parameters:\n",
    "        if(str(key)[0] == \"W\"):\n",
    "            matrix_sum += np.sum(np.square(parameters[key]))\n",
    "        \n",
    "    L2_regularization_cost = (lambd/(2*m))*(matrix_sum)\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, test_x, test_y, lambd, learning_rate = 0.007, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = [] # keep track of cost\n",
    "    prediction_test = [] # keep track of the prediction accuracy\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_he(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        if(i % 300 == 0 and i != 0):\n",
    "            learning_rate /= 2\n",
    "            print(\"Learning rate: \", learning_rate)\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        # Compute cost.\n",
    "        cost = compute_cost_with_regularization(AL, Y, parameters, lambd)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backward_with_regularization(AL, Y, caches, lambd)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            \n",
    "            p, accuracy = predict(test_x, test_y, parameters) #predict the test_data\n",
    "            print(\"Accuracy: {0}\".format(accuracy))\n",
    "            \n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))            \n",
    "            print(\"---------------------------------------\")\n",
    "            \n",
    "            costs.append(cost)\n",
    "            prediction_test.append(float(accuracy))\n",
    "        \n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.plot(np.squeeze(prediction_test), 'ro')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 20, 20, 15, 10, 7, 5, 1] #  8-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6323232323232322\n",
      "Cost after iteration 0: 0.664653\n",
      "---------------------------------------\n",
      "Accuracy: 0.6828282828282827\n",
      "Cost after iteration 100: 0.574037\n",
      "---------------------------------------\n",
      "Accuracy: 0.715151515151515\n",
      "Cost after iteration 200: 0.524664\n",
      "---------------------------------------\n",
      "Learning rate:  0.0375\n",
      "Accuracy: 0.7373737373737372\n",
      "Cost after iteration 300: 0.605745\n",
      "---------------------------------------\n",
      "Accuracy: 0.7595959595959594\n",
      "Cost after iteration 400: 0.436916\n",
      "---------------------------------------\n",
      "Accuracy: 0.7656565656565655\n",
      "Cost after iteration 500: 0.424874\n",
      "---------------------------------------\n",
      "Learning rate:  0.01875\n",
      "Accuracy: 0.7555555555555553\n",
      "Cost after iteration 600: 0.501145\n",
      "---------------------------------------\n",
      "Accuracy: 0.7515151515151514\n",
      "Cost after iteration 700: 0.332001\n",
      "---------------------------------------\n",
      "Accuracy: 0.7515151515151514\n",
      "Cost after iteration 800: 0.322743\n",
      "---------------------------------------\n",
      "Learning rate:  0.009375\n",
      "Accuracy: 0.7373737373737372\n",
      "Cost after iteration 900: 0.412314\n",
      "---------------------------------------\n",
      "Accuracy: 0.7414141414141413\n",
      "Cost after iteration 1000: 0.230365\n",
      "---------------------------------------\n",
      "Accuracy: 0.7434343434343433\n",
      "Cost after iteration 1100: 0.205056\n",
      "---------------------------------------\n",
      "Learning rate:  0.0046875\n",
      "Accuracy: 0.7434343434343432\n",
      "Cost after iteration 1200: 0.173914\n",
      "---------------------------------------\n",
      "Accuracy: 0.7393939393939392\n",
      "Cost after iteration 1300: 0.139800\n",
      "---------------------------------------\n",
      "Accuracy: 0.7373737373737372\n",
      "Cost after iteration 1400: 0.119788\n",
      "---------------------------------------\n",
      "Learning rate:  0.00234375\n",
      "Accuracy: 0.7353535353535352\n",
      "Cost after iteration 1500: 0.106851\n",
      "---------------------------------------\n",
      "Accuracy: 0.7313131313131311\n",
      "Cost after iteration 1600: 0.092531\n",
      "---------------------------------------\n",
      "Accuracy: 0.7272727272727271\n",
      "Cost after iteration 1700: 0.085407\n",
      "---------------------------------------\n",
      "Learning rate:  0.001171875\n",
      "Accuracy: 0.723232323232323\n",
      "Cost after iteration 1800: 0.079603\n",
      "---------------------------------------\n",
      "Accuracy: 0.723232323232323\n",
      "Cost after iteration 1900: 0.076809\n",
      "---------------------------------------\n",
      "Accuracy: 0.723232323232323\n",
      "Cost after iteration 2000: 0.074299\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, test_x, test_y, 0.05, num_iterations = 2000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9152542372881354\n"
     ]
    }
   ],
   "source": [
    "pred_train, acc = predict(train_x, train_y, parameters)\n",
    "print(\"Acc: \", acc) # 0.9880358923230306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.7454545454545454\n"
     ]
    }
   ],
   "source": [
    "pred_test, acc = predict(test_x, test_y, parameters)\n",
    "print(\"Acc: \", acc) # 0.7414141414141412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of FALSE POSITIVES were:  86\n",
      "The number of FALSE NEGATIVES were:  60\n",
      "The number of TRUE POSITIVES were:  237\n",
      "The number of TRUE NEGATIVES were:  110\n"
     ]
    }
   ],
   "source": [
    "analysis = false_and_positives_analysis(pred_test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
